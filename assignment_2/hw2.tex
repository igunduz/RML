\documentclass[11pt]{article}
\usepackage{macro} % replace this line with the one below for your submission
% \usepackage[response]{macro}

\begin{document}

\makeheader

\makemytitle{Homework Assignment 2}


\submitter{TODO: replace this with your name (and Matr. ID)}
\due{2:00pm Thursday, 7 December 2023 on CISPA CMS}

\directions{
The purpose of this assignment is to get you familiar with black-box attacks, empirical defenses such as adversarial training and certified defenses such as randomized smoothing introduced in Lectures 4-5. 
}

\collaboration{You should do this assignment by yourself and submit your own answers. You may discuss the problems with anyone you want and it is also fine to get help from anyone on problems with LaTeX or Jupyter/Python. You should note in the {\em Collaborators} box below the people you collaborated with.}


\collaborators{TODO: replace this with your collaborators (if you did not have any, replace this with {\em None})}

\directions{
This problem set includes both PDF and Jupyter notebook components. You should complete the answers to the PDF part by writing your answers in \texttt{hw2.tex}, and submitting your generated PDF file in CISPA CMS under Submission tab. Similar to the first assignment, the first thing you should do in \keyword{hw2.tex} is setting up your name as the author of the submission by replacing the line, \texttt{\textbackslash submitter\{TODO: your name\}}, with your name and your Matr. ID, e.g., \texttt{\textbackslash submitter\{Susan Blake (7583916)\}}. Before submitting your PDF, also remember to (1) list your collaborators by replacing the TODO in {\texttt{\textbackslash collaborators\{TODO: replace ...\}}}, and (2) replace the second line in \keyword{hw2.tex}, \texttt{\textbackslash usepackage\{macro\}} with \texttt{\textbackslash usepackage[response]\{macro\}} so the directions do not appear in your final PDF.
}


\begin{problem}[10 pts]
\label{problem:1}
\rm
Consider the same setting of binary logistic regression as in Problems 1-2 of Homework Assignment 1. Recall from Lecture 4, adversarial training aims to solve the following min-max optimization problem using projected gradient descent (PGD) in the context of logistic regression:
\begin{align}
\label{eq:adv train obj logistic regression}
    \min_{\bw} \EE_{(\bx,y)\sim\mu} \bigg[\max_{\bx'\in\cB_\epsilon(\bx, \ell_\infty)} -\log \sigma\big(y\cdot \langle \bw, \bx' \rangle\big) \bigg], 
\end{align}
where $\mu$ represents the underlying data distribution. \textbf{Write down the pseudocode of PGD-based adversarial training algorithm.} The input of the algorithm should be a set of $m$ training examples $\{(\bx_i, y_i)\}_{i\in[m]}$ sampled from $\mu$ and all the necessary hyperparameters, such as perturbation budget $\epsilon$, attack step size $\alpha$, number of attack steps $S$, learning rate $\eta$, number of training epochs $T$, batch size $B$ and etc. The output should be a weight vector $\widehat{\bw}$ that is supposed to be a good solution to problem \eqref{eq:adv train obj logistic regression}.
\end{problem}

\directions{
\textbf{Note:} You may want to look into the original paper \cite{madry2017towards} that puts forward the algorithm of PGD-based adversarial training for specific algorithmic/implementation details.
}



\directions{\clearpage}



\begin{problem}[10 pts]
\label{problem:2}
\rm
Suppose $f$ is a two-class linear classifier with parameters $\bw\in\RR^d$ and $b\in\RR$, where $f(\bx) = \sgn\big(\langle \bw, \bx \rangle + b\big)$ for any $\bx\in\RR^d$ and $\sgn(\cdot)$ is the sign function.
Suppose $g$ represents the smoothed version of $f$ used for randomized smoothing. Specifically for any $\bx\in\RR^d$, $g$ is defined as:
\begin{align*}
    g(\bx) = \argmax_{j\in\{-1, +1\}} \PP_{\bm\delta}\bigg[ f(\bx+\bm\delta) = j\bigg], \text{ where } \bm\delta \sim\cN(\bm{0}, \sigma^2\mathbf{I}),
\end{align*}
where $\sigma>0$ is the smoothing parameter. Prove that the following two statements:
\begin{enumerate}
    \item[1.] (5 pts) For any input $\bx\in\RR^d$, $g(\bx) = f(\bx)$.
    \item[2.] (5 pts) For any input $\bx\in\RR^d$, the certified radius of $g$ at $\bx$ with $\ell_2$-norm is $R(\bx) = \frac{\big|\langle \bw, \bx \rangle + b\big|}{\|\bw\|_2}$. In other words, $g(\bx+\bm\delta)$ remains the same for any $\bm\delta\in\RR^d$ with $\|\bm\delta\|_2\leq R(\bx)$.
\end{enumerate}
\end{problem}

\directions{
\textbf{Note:} The first result implies that the smoothed classifier $g$ is by design identical to the base classifier $f$, provided that the base classifier $f$ is a two-class linear model, while the second results suggests that the certified radius defined by Theorem 1 in \cite{cohen2019certified} is always ``tight'' for two-class linear classifiers.
}

\directions{\clearpage}

\shortsection{Implementation Problem} Below is an implementation problem that you need to complete the provided Jupyter notebook. More specifically, you will need to implement both standard training and PGD-based adversarial training using the MNIST handwritten digits dataset. Then, you will evaluate the learned models against different adversarial attacks.

\directions{
If you haven't used Jupyter notebook before, you can start by installing Jupyter on your computer using this link: \url{https://jupyter.org/install}. To run the provided \texttt{hw2.ipynb} file (especially for the PGD-based adversarial training part) efficiently, it is necessary to use a GPU instead of CPU. The easiest way is to use the free GPU on Google Collab to run the jupyter notebook. If you happen to have access to more powerful GPUs, you are also suggested to use them for fast computation, but the free GPU provided by Google Collab should be sufficient for you to complete this homework assignment.
}


\begin{problem}[20 pts]
\label{problem:3}
\rm
Consider the classification task on MNIST and $\ell_\infty$ perturbations with $\epsilon=0.1$. Suppose we want to train a CNN model using standard deep learning and a robust CNN model using PGD-based adversarial training. The CNN architecture has $4$ convolutional layers and $2$ MLP layers. We use a SGD optimizer with learning rate $0.1$ for both methods, and both models are trained for $5$ epochs.

\textbf{Your Task:} Complete the corresponding functions in the provided Jupyter notebook. Report the test classification errors of both standard-trained and adversarially-trained models in terms of no attack (clean), FGSM attack and PGD attack, respectively, and discuss the results.


\end{problem}

\directions{
\textbf{Note:} The hyperparameters for PGD attack is set as attack step size \texttt{alpha=0.02}, number of PGD steps \texttt{num_iter=10}. The option \texttt{randomize} is for the initialization scheme for PGD-based adversarial training: if \texttt{randomize=False}, we use a zero-initialization scheme like FGSM; otherwise, we use a random-initialization scheme. You can choose to use either option of the two initialization schemes.
}



\directions{\clearpage}


\begin{problem}[bonus, 10 pts]
\label{problem:bonus}
\rm
Your task is to develop a function \text{my_attack()} that can produce more adversarial examples for MNIST testing examples (i.e., a higher attack success rate) in the Jupyter notebook. Your attack should be valid in the sense that it only produces adversarial perturbations within the $\ell_\infty$-norm ball with $\epsilon=0.1$. Also, you need to briefly describe how your attack methodology is developed and what is the attack success rate (ASR) it can achieve.


\end{problem}


\directions{
\textbf{Note:} You may want to implement different attack methods that we learned during class, or varying the hyperparameters for different methods. 
The higher the attack success rates it can achieve, the higher bonus credits you may receive.
}


\vspace{20pt}


\begin{center}
{\bf End of Homework Assignment 2 (PDF part)} \\
Don't forget to also complete and submit the Jupyter notebook!
\end{center}


\clearpage

\bibliography{rob.bib}
\bibliographystyle{alpha}


\end{document}
